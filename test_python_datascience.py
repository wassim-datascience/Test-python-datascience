# -*- coding: utf-8 -*-
"""Test-python-datascience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CoSv7hlTJj7URPqE5mW0ntNUhKPx0G0X

# Introduction:

SensCritique souhaite enrichir l’expérience de ses utilisateurs en leur proposant, lors de la lecture d’une critique, d’accéder à d’autres critiques similaires.
Un tel système de recommandation basé sur le texte vise à améliorer la navigation, à favoriser la découverte de points de vue variés et à renforcer l’engagement sur la plateforme.

L’objectif de ce projet est donc de développer un prototype d’algorithme capable d’identifier des critiques proches les unes des autres, en se basant uniquement sur leur contenu textuel.
La démarche suivie peut se résumer ainsi :

- **Exploration et préparation des données** : nettoyage des critiques, traitement des valeurs manquantes, concaténation des titres et du contenu.  
- **Représentation vectorielle des textes** : utilisation de plusieurs approches, du TF-IDF classique aux représentations par *word embeddings* et *sentence embeddings*.  
- **Mesure de similarité** : calcul de proximités entre critiques, principalement à l’aide de la similarité cosinus.  
- **Évaluation qualitative** : analyse des résultats obtenus en fonction de la pertinence perçue et en veillant à limiter les “faux positifs” (par exemple des rapprochements entre films différents).

Cette étude ne vise pas à livrer un système complet de production, mais à proposer une solution simple, reproductible et évolutive, qui puisse servir de base à un système de recommandation de critiques.

# Chargement + exploration des données:

Dans cette section on propose d'explorer rapidement les données notamment en vu de voir à quoi elles ressemblent et de détécter l'éventuelle présence de valeurs manquantes.
"""

import pandas as pd
from google.colab import files

import numpy as np

# On charge les deux csv dans Google Colab
uploaded = files.upload()

# On charge les deux csv dans un dataFrame chacun
df_fc = pd.read_csv("fightclub_critiques.csv")
df_is = pd.read_csv("interstellar_critiques.csv")

# On explore un peu les données pour voir à quoi elles ressemblent et voir eventuellement
# la présence ou non de valeur manquante
print(df_fc.head(10))
print(df_fc.shape , df_is.shape)

# chercher les valeurs manquantes pour Fight Club
missing_values_fc = df_fc.isnull().sum()
missing_values_fc.head(20)

# chercher les valeurs manquantes pour Interstellar
missing_values_is = df_is.isnull().sum()
missing_values_is.head(20)

"""Dans le csv Fight Club on remarque la présence de 1 article vide + 351 titres vides.
Dans le csv Interstellar on remrque la présence de 168 titres vide.
On propose de supprimer l'article vide.
Vu qu'on compte mettre le titre et l'article dans une même variable text, on remplacera les titres vides par une chaine de caractère vide en vue de la concaténation.

# Preprocessing des données:
"""

df_fc = df_fc.dropna(subset=["review_content"]) # suppression de l'article vide

# On ajoute colonne 'film'
df_fc["film"] = "Fight Club"
df_is["film"] = "Interstellar"

df_all = pd.concat([df_fc, df_is], ignore_index=True) # concaténation des deux tables

df_all["review_title"] = df_all["review_title"].fillna("") # remplacement des titres vides par ""
df_all["text"] = df_all["review_title"] + " " + df_all["review_content"] # Ajout du titre à l'article
df_all[["review_title", "review_content", "text"]].head() # On regarde le résultat

import re
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
import nltk

# on charge les stopwords français de NLTK
nltk.download("stopwords")
stop_words = set(stopwords.words("french"))

def clean_text(text):
    # Supprimer HTML
    text = BeautifulSoup(text, "html.parser").get_text()
    # Mettre en minuscules
    text = text.lower()
    # Supprimer tout sauf lettres et espaces
    text = re.sub(r"[^a-zàâçéèêëîïôûùüÿñæœ\s]", " ", text)
    # Supprimer les stopwords
    tokens = [word for word in text.split() if word not in stop_words]
    return " ".join(tokens)

df_all["text"] = df_all["text"].apply(clean_text)
df_all["text"].head()

"""# Métrique d'évaluation:

Ici on est devant une problematique d'apprentissage non suppérvisé et donc de ce fait on ne peut pas évaluer objectivement nos différents models à moins d'avoir une base de critiques annotés avec la similarités entres les différents articles. Une des approches peut être d'annoter manuellement un petit échantillon de critiques, ce qui est quand meme un travail assez laborieux, et on pense que ce n'est pas forcement ce qui est attendu de notre part lors de ce test. Une deuxieme approche aurait pu être de tester nos résultat avec le systeme de recommendations en vigueur sur le site de sens critique mais vu que on n'a que des extraits de l'ensemble des critiques pour les deux films, une telle approche sera infrutueuse. C'est pour celà que je propose d'utiliser le csv Interstellar pour voir déjà à quel point nos differents models vont nous proposer des critique pas du tout du même film. Cette metrique ne prends biensur pas en compte une vrai similarité de contexte mais peut être une première approche pour évaluer nos différents models.
"""

# Ici on code la fonction accuracy_at_k qui va nous permettre d'évaluer nos différents models

from sklearn.metrics.pairwise import cosine_similarity

def accuracy_at_k(matrix, labels, k=5, n_samples=200):
    """
    matrix : matrice d'embeddings (ex. TF-IDF, FastText, SBERT , CamemBERT)
    labels : liste/colonne avec le film correspondant
    k      : nombre de voisins à recommander
    n_samples : nombre de critiques à tester (aléatoire)
    """
    from random import sample

    idx_sample = sample(range(matrix.shape[0]), n_samples)
    accs = []

    sims = cosine_similarity(matrix)  # matrice NxN de similarités

    for idx in idx_sample:
        # voisins triés du plus proche au moins proche
        neighbors = np.argsort(sims[idx])[::-1]
        neighbors = neighbors[1:k+1]  # enlever soi-même (idx)

        # vérifier combien ont le même film
        correct = sum(labels[n] == labels[idx] for n in neighbors)
        accs.append(correct / k)

    return np.mean(accs)

"""# Approche TF-IDF:

TF-IDF : compare les critiques mot par mot, pondérés par la fréquence. C'est l'approche la plus basique mais la plus rapide. C'est un bon point de départ pour pouvoir voir comment se comporte notre corpus et ce que peuvent apporter les models plus développés/modernes.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer() # on charge le model TFIDF

tfidf_matrix = vectorizer.fit_transform(df_all["text"]) # on vectorise notre text

# Application de l'accuracy_at_5 à TFIDF

labels = df_all["film"].values
acc_tfidf = accuracy_at_k(tfidf_matrix, labels, k=5, n_samples=200)
print(f"Accuracy@5 intra-film avec TF-IDF : {acc_tfidf:.2f}")

"""Ici on a obtenu un bon résultat avec notre métrique précision à 5 (accuracy_at_5). Ceci s'explique par le fait que tfidf repose exclusivement sur la distribution des mots du vocabulaire dans les articles et de ce fait est fortement biaisé par la présence de certains mots caractéristiques à chaque film (noms acteurs, film...). Cependant on si on avait une base de donnée annoté selon les similarités, ce model ne devrait pas bien performer car il ne tient pas compte du context et de la sémantique des mots.

# Approche Word Embedding: FastText

Word Embeddings (Word2Vec / FastText) : chaque mot → vecteur, puis on fait une moyenne des mots pour représenter une critique. Vu que, à ce qu'on sache, il n'existe pas de model Word2Vec pré-entrainé sur des corpus en Français. On propose d'utiliser FastText qui est déjà une amélioration de Word2Vec tout en ayant une implémentation préentrainé sur du Français.
"""

# On télécharge un model fastext préentrainé sur du français:

!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz
!gunzip cc.fr.300.vec.gz

# j'installe gensim car il est absent ici
!pip install gensim

import gensim
import numpy as np

# Charger FastText (version réduite dans ce notebook car la version entière consomme beaucoup de RAM)
model = gensim.models.KeyedVectors.load_word2vec_format("cc.fr.300.vec", binary=False, limit=200000)
# model = gensim.models.KeyedVectors.load_word2vec_format("cc.fr.300.vec", binary=False)

def document_vector(doc, model):
    words = [w for w in doc.split() if w in model]
    if len(words) == 0:
        return np.zeros(model.vector_size)
    return np.mean([model[w] for w in words], axis=0)

# Construire les vecteurs pour toutes les critiques
doc_vectors_ft = np.array([document_vector(text, model) for text in df_all["text"]])

# Accuracy@5 avec FastText
labels = df_all["film"].values
acc_ft = accuracy_at_k(doc_vectors_ft, labels, k=5, n_samples=200)
print(f"Accuracy@5 intra-film avec FastText : {acc_ft:.2f}")

"""ici FastText réduit m'a donné une précision de 66% et lorsque j'ai executé le model entier (qui est long et dépasse parfois la limite de ma RAM) j'ai obtenu une précision de 72%. Ce résultat, plus faible que l'approche TFIDF sur cette métrique en particulier, ne devrait pas trop nous surprendre, car certes tfidf est plus basique comme approche, mais elle se base sur la similarité de distribution de certain mot du vocabulaire dans les articles. Donc dans des articles où les noms des films et des acteurs sont mentionnés à plusieurs reprise c'est tout à fait prévisible que l'approche tfidf va nous proposer des articles du meme film à chaque fois. Or l'approche FastText qui est une approche Word embedding va surement enlever les noms des acteurs et meme parfois des films (s'ils ne figurent pas dans le vocabulaire). Néanmoins si on avait une base annoté selon les similarités on devrait voir, comme dans les benchmarks dans la littérature, un résultat à l'avantage de FastText.

# Approche Sentence embedding: SBERT

Ici on passe à ce que l'état de l'art propose de mieux pour representer nos articles. A l'opposer de l'approche Word Embedding où on est obligé de faire une moyenne sur les représentations de chaque mot pour représenter nos articles et donc de ce fait on perd la structure et le sens global. Le sentence embedding (ici SBERT) génére un vecteur pour tout l'article en tenant compte de la syntaxe et du context, et de ce fait, on est sensé avoir une forte similarité entre les articles proposé.

Ici on propose d'utiliser un model multilangues préentrainé de SBERT, contenant biensur le Français : "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
"""

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer

# Charger SBERT multilingue
sbert_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# Encoder toutes les critiques
doc_vectors_sbert = sbert_model.encode(df_all["text"].tolist(), show_progress_bar=True)

# Accuracy@5 avec SBERT
labels = df_all["film"].values
acc_sbert = accuracy_at_k(doc_vectors_sbert, labels, k=5, n_samples=200)
print(f"Accuracy@5 intra-film avec SBERT : {acc_sbert:.2f}")

"""On remarque, sans surprise que SBERT surperforme tous les autres models (score 93%), sur notre métrique proposé. En plus les critiques proposés sont nettement plus similaires, au sens demandé par le sujet du test, et ceci est dû de façon inhérente à la construction et l'objectif meme du model.

Vu que le model SBERT proposé est un model multilangues, on se doute bien qu'il a quand meme quelques limites par rapport à un model spécialement concu pour le Français (ex: CamemBERT, FlauBERT)

# CamemBERT:

C’est la version française de BERT, entraînée sur des milliards de mots.

Plus adaptée que SBERT multilingue, car elle capte mieux les subtilités du français.

Le problème c'est que CamemBERT n’est pas directement un model de sentence embedding. il faut soit :

- Faire la moyenne des tokens

- Soit passer par CamemBERT + Sentence-Transformers (Comme ça le sentence-transformers opérera sur une très riche/fine représentation des tokens français tout en effectuant un vrai Sentence Embedding ). On prévoit que ce model va être le mieux adapté à notre problématique poser par le sujet du test.

Camembert Seul:
"""

!pip install -q transformers torch

from transformers import CamembertTokenizer, CamembertModel
import torch
import numpy as np

# Charger tokenizer + modèle CamemBERT
tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
camembert = CamembertModel.from_pretrained("camembert-base")
camembert.eval()  # mode évaluation

def camembert_encode(text, model, tokenizer, max_length=128):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=max_length)
    with torch.no_grad():
        outputs = model(**inputs)
    # outputs.last_hidden_state : (batch_size, seq_len, hidden_size)
    embeddings = outputs.last_hidden_state.squeeze(0).numpy()
    return np.mean(embeddings, axis=0)  # moyenne sur les tokens

# Limiter à un sous-échantillon si c’est trop lourd au début
texts = df_all["text"].tolist()

doc_vectors_camembert = np.array([camembert_encode(t, camembert, tokenizer) for t in texts])

acc_camembert = accuracy_at_k(doc_vectors_camembert, labels, k=5, n_samples=200)
print(f"Accuracy@5 intra-film avec CamemBERT (moyenne tokens) : {acc_camembert:.2f}")

"""Camembert + transformers:

lors de l'utilisation de cette approche j'ai rencontré plusieurs problèmes pour avoir un code qui fonctionne. Ceci peut être un probleme de ma part pour ne pas avoir trouver où ce situe l'erreur ou un package qui présente encore des manquements notament pour traiter les très longs articles.

# Conclusion:

Ce travail a permis de mettre en place un prototype fonctionnel de système de recommandation de critiques similaires.
Les différentes méthodes testées montrent des résultats contrastés :

- **TF-IDF** offre une solution rapide et efficace, capable de capturer les mots saillants mais limitée lorsqu’il s’agit de saisir le sens profond des phrases.  
- **Word embeddings** enrichissent la représentation en tenant compte des relations sémantiques entre les mots, mais leur performance dépend fortement de la qualité des vecteurs utilisés.  
- **Sentence embeddings** apportent un gain significatif en permettant une représentation contextuelle et globale de chaque critique, souvent plus pertinente pour ce type de tâche.

Même si l’évaluation reste qualitative, les résultats obtenus confirment la faisabilité d’un tel système.  
Des améliorations futures pourraient inclure :
- la mise en place d’un échantillon annoté pour une évaluation quantitative,  
- l’intégration de modèles de type **transformers appliqués sur (BERT, DistilBERT)** pour des représentations plus fines,  
- l’ajout de métadonnées (auteur, note attribuée, date, etc.) pour enrichir la recommandation.

En résumé, cette approche démontre qu’il est possible d’identifier efficacement des critiques similaires à partir de leur contenu textuel et constitue une base solide pour le développement d’un système de recommandation au sein de SensCritique.
"""